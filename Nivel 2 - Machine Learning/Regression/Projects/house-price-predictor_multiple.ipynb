{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aaf1899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad value in file './deeplearning.mplstyle', line 6 ('axes.edgecolor : #4f4f4f'): Key axes.edgecolor: '' does not look like a color arg\n",
      "Bad value in file './deeplearning.mplstyle', line 7 ('axes.labelcolor : #4f4f4f'): Key axes.labelcolor: '' does not look like a color arg\n",
      "Bad value in file './deeplearning.mplstyle', line 17 ('xtick.color : #4f4f4f'): Key xtick.color: '' does not look like a color arg\n",
      "Bad value in file './deeplearning.mplstyle', line 19 ('ytick.color : #4f4f4f'): Key ytick.color: '' does not look like a color arg\n",
      "Bad value in file './deeplearning.mplstyle', line 6 ('axes.edgecolor : #4f4f4f'): Key axes.edgecolor: '' does not look like a color arg\n",
      "Bad value in file './deeplearning.mplstyle', line 7 ('axes.labelcolor : #4f4f4f'): Key axes.labelcolor: '' does not look like a color arg\n",
      "Bad value in file './deeplearning.mplstyle', line 17 ('xtick.color : #4f4f4f'): Key xtick.color: '' does not look like a color arg\n",
      "Bad value in file './deeplearning.mplstyle', line 19 ('ytick.color : #4f4f4f'): Key ytick.color: '' does not look like a color arg\n",
      "Bad value in file './deeplearning.mplstyle', line 6 ('axes.edgecolor : #4f4f4f'): Key axes.edgecolor: '' does not look like a color arg\n",
      "Bad value in file './deeplearning.mplstyle', line 7 ('axes.labelcolor : #4f4f4f'): Key axes.labelcolor: '' does not look like a color arg\n",
      "Bad value in file './deeplearning.mplstyle', line 17 ('xtick.color : #4f4f4f'): Key xtick.color: '' does not look like a color arg\n",
      "Bad value in file './deeplearning.mplstyle', line 19 ('ytick.color : #4f4f4f'): Key ytick.color: '' does not look like a color arg\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import math, copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from lab_utils_uni import plt_intuition, plt_stationary, plt_update_onclick, soup_bowl\n",
    "import time\n",
    "plt.style.use('./deeplearning.mplstyle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc967118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters   \n",
    "      b (scalar):             model parameter \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    p = np.dot(x, w) + b     \n",
    "    return p  \n",
    "\n",
    "\n",
    "def compute_cost(X, y, w, b): \n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):                                \n",
    "        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)\n",
    "        cost = cost + (f_wb_i - y[i])**2       #scalar\n",
    "    cost = cost / (2 * m)                      #scalar    \n",
    "    return cost\n",
    "\n",
    "\n",
    "def compute_cost_vectorized(X, y, w, b): \n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    f_wb = np.dot(X, w) + b\n",
    "    err = f_wb - y\n",
    "    cost = np.sum(err**2) / (2 * m)\n",
    "    return cost\n",
    "\n",
    "\n",
    "\n",
    "def compute_gradient(X, y, w, b): \n",
    "    m, n = X.shape          \n",
    "    dj_dw = np.zeros((n,))  \n",
    "    dj_db = 0.               \n",
    "    for i in range(m):                             # Loop pelos exemplos de treino\n",
    "        # 1. Calcula o erro da previsão para este exemplo\n",
    "        # f_wb é o \"palpite\" do modelo (combinação de todos os x com os w)\n",
    "        err = (np.dot(X[i], w) + b) - y[i]   \n",
    "        # 2. Calcula o impacto desse erro em cada peso w_j\n",
    "        for j in range(n):                         # Loop pelas características\n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]    # Multiplica erro pela característica j\n",
    "            \n",
    "        # 3. O erro de b não depende de nenhuma característica x\n",
    "        dj_db = dj_db + err                        \n",
    "    # No final, divide pela média (m)\n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m                                \n",
    "        \n",
    "    return dj_db, dj_dw\n",
    "\n",
    "\n",
    "def compute_gradient_vectorized(X, y, w, b):\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # 1. Calcula o erro para todas as linhas de uma vez (vetor de tamanho m)\n",
    "    f_wb = X @ w + b\n",
    "    err = f_wb - y\n",
    "    \n",
    "    # 2. Calcula o gradiente de w (vetor de tamanho n)\n",
    "    # Multiplica a transposta de X pelo erro e divide por m\n",
    "    dj_dw = (1/m) * (X.T @ err)\n",
    "    \n",
    "    # 3. Calcula o gradiente de b (escalar)\n",
    "    dj_db = np.sum(err) / m\n",
    "    \n",
    "    return dj_db, dj_dw\n",
    "\n",
    "    \n",
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "\n",
    "\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        dj_db, dj_dw = gradient_function(X, y, w, b)\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "      \n",
    "        if i < 100000:      \n",
    "            J_history.append(cost_function(X, y, w, b))\n",
    "        if i % math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}\")\n",
    "        \n",
    "    return w, b, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7db439b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho dos dados de treinamento: (20640, 4)\n",
      "Gradient descent sem vetorização\n",
      "Iteration    0: Cost     2.80\n",
      "Iteration  100: Cost     2.77\n",
      "Iteration  200: Cost     2.73\n",
      "Iteration  300: Cost     2.69\n",
      "Iteration  400: Cost     2.65\n",
      "Iteration  500: Cost     2.62\n",
      "Iteration  600: Cost     2.58\n",
      "Iteration  700: Cost     2.55\n",
      "Iteration  800: Cost     2.51\n",
      "Iteration  900: Cost     2.48\n",
      "[FOR LOOP] Tempo: 101.3458 segundos\n",
      "Gradient descent com vetorização\n",
      "Iteration    0: Cost     2.80\n",
      "Iteration  100: Cost     2.77\n",
      "Iteration  200: Cost     2.73\n",
      "Iteration  300: Cost     2.69\n",
      "Iteration  400: Cost     2.65\n",
      "Iteration  500: Cost     2.62\n",
      "Iteration  600: Cost     2.58\n",
      "Iteration  700: Cost     2.55\n",
      "Iteration  800: Cost     2.51\n",
      "Iteration  900: Cost     2.48\n",
      "[FOR LOOP] Tempo: 0.0879 segundos\n",
      "b,w found by gradient descent: 0.00,[0.00091759 0.00577627 0.00111898 0.0002146 ] \n"
     ]
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "x_train = housing.data[:, :4]  # Pega as colunas 0, 1, 2 e 3\n",
    "y_train = housing.target\n",
    "print(f\"Tamanho dos dados de treinamento: {x_train.shape}\")\n",
    "\n",
    "\n",
    "initial_w = np.zeros(x_train.shape[1])\n",
    "initial_b = 0.\n",
    "\n",
    "# configurações para a descida gradiente\n",
    "iterations = 1000\n",
    "alpha = 1.0e-7\n",
    "\n",
    "\n",
    "print(\"Gradient descent sem vetorização\")\n",
    "start = time.time()\n",
    "# ... rodar versao loop ...\n",
    "# descida gradiente sem vetorização\n",
    "w_final, b_final, J_hist = gradient_descent(x_train, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    alpha, iterations)\n",
    "end = time.time()\n",
    "print(f\"[FOR LOOP] Tempo: {end - start:.4f} segundos\")\n",
    "\n",
    "print(\"Gradient descent com vetorização\")\n",
    "start = time.time()\n",
    "# descida gradiente com vetorização\n",
    "w_final, b_final, J_hist = gradient_descent(x_train, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost_vectorized, compute_gradient_vectorized, \n",
    "                                                    alpha, iterations)\n",
    "end = time.time()\n",
    "print(f\"[FOR LOOP] Tempo: {end - start:.4f} segundos\")\n",
    "\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "\n",
    "# m,_ = x_train.shape\n",
    "#for i in range(m):\n",
    "#   print(f\"prediction: {np.dot(x_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
